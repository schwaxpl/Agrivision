"""
Processeur pour extraire des informations structur√©es d'articles scientifiques
√† partir de documents markdown en utilisant LangChain.
"""

import os
from typing import List, Optional, Dict, Any
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.language_models.base import BaseLanguageModel
from langchain_mistralai import ChatMistralAI
import logging

from ..models.scientific_article import ScientificArticle
from ..config import config
from ..config import config


class ScientificArticleProcessor:
    """
    Processeur pour extraire des informations structur√©es d'articles scientifiques
    √† partir de r√©sum√©s en markdown.
    """
    
    def __init__(self, 
                 llm: Optional[BaseLanguageModel] = None,
                 temperature: Optional[float] = None,
                 model_name: Optional[str] = None):
        """
        Initialise le processeur.
        
        Args:
            llm: Mod√®le de langage √† utiliser (si None, utilise ChatMistralAI par d√©faut)
            temperature: Temp√©rature pour le mod√®le (utilise config par d√©faut si None)
            model_name: Nom du mod√®le √† utiliser (utilise config par d√©faut si None)
        """
        if llm is None:
            # Configuration avec Mistral en utilisant les variables d'environnement
            model_config = config.get_model_config()
            
            self.llm = ChatMistralAI(
                model=model_name or model_config["model"],
                temperature=temperature or model_config["temperature"],
                max_tokens=model_config["max_tokens"],
                timeout=model_config["timeout"],
                mistral_api_key=config.MISTRAL_API_KEY
            )
        else:
            self.llm = llm
            
        # Parser pour convertir la sortie en objet Pydantic
        self.output_parser = PydanticOutputParser(pydantic_object=ScientificArticle)
        
        # Template de prompt pour l'extraction
        self.prompt_template = self._create_prompt_template()
        
        # Cha√Æne de traitement LangChain
        self.chain = self.prompt_template | self.llm | self.output_parser
    
    def _create_prompt_template(self) -> PromptTemplate:
        """
        Cr√©e le template de prompt pour l'extraction d'informations.
        
        Returns:
            Template de prompt LangChain
        """
        template = """
Vous √™tes un expert en analyse d'articles scientifiques. 
Votre t√¢che est d'extraire des informations structur√©es √† partir du r√©sum√© d'un article scientifique fourni en format markdown.

Analysez attentivement le texte suivant et extrayez les informations demand√©es. 
Si une information n'est pas disponible ou ne peut pas √™tre d√©termin√©e avec certitude, laissez le champ vide ou utilisez une valeur par d√©faut appropri√©e.

TEXTE √Ä ANALYSER:
{text}

INSTRUCTIONS:
- Identifiez le titre de l'article
- Extrayez la liste des auteurs (s'ils sont mentionn√©s)
- R√©cup√©rez le r√©sum√©/abstract complet
- Identifiez les mots-cl√©s pertinents
- D√©terminez la date de publication si mentionn√©e
- Identifiez le journal/revue de publication si mentionn√©
- Trouvez le DOI si pr√©sent
- D√©terminez le domaine de recherche principal
- Identifiez la m√©thodologie utilis√©e
- Extrayez les principales d√©couvertes/r√©sultats
- √âvaluez votre confiance dans l'extraction (0.0 √† 1.0)

{format_instructions}

R√âPONSE:
"""
        
        return PromptTemplate(
            template=template,
            input_variables=["text"],
            partial_variables={
                "format_instructions": self.output_parser.get_format_instructions()
            }
        )
    
    def process_document(self, document: Document) -> ScientificArticle:
        """
        Traite un document et extrait les informations structur√©es.
        
        Args:
            document: Document LangChain contenant le texte markdown
            
        Returns:
            Objet ScientificArticle avec les informations extraites
            
        Raises:
            Exception: Si le traitement √©choue
        """
        try:
            # Debug: afficher le prompt si demand√©
            if config.SHOW_PROMPTS:
                prompt_text = self.prompt_template.format(text=document.page_content[:500] + "...")
                print(f"üîç PROMPT ENVOY√â AU MOD√àLE:\n{prompt_text}\n{'='*60}")
            
            # Ex√©cution de la cha√Æne LangChain
            result = self.chain.invoke({"text": document.page_content})
            
            # Debug: sauvegarder la r√©ponse brute si demand√©
            if config.SAVE_RAW_RESPONSES:
                import json
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                response_file = f"{config.LOG_DIR}/raw_response_{timestamp}.json"
                with open(response_file, 'w', encoding='utf-8') as f:
                    json.dump(result.model_dump() if hasattr(result, 'model_dump') else result.dict(), f, indent=2, default=str)
            
            # Ajout des m√©tadonn√©es du document source
            if hasattr(result, 'model_fields_set'):
                # Pour les versions r√©centes de Pydantic
                result_dict = result.model_dump()
            else:
                result_dict = result.dict()
                
            # Ajouter des m√©tadonn√©es sur la source
            if document.metadata:
                result_dict["source_metadata"] = document.metadata
            
            return result
            
        except Exception as e:
            if config.DEBUG_MODE:
                import traceback
                traceback.print_exc()
            raise Exception(f"Erreur lors du traitement du document: {str(e)}")
    
    def process_documents(self, documents: List[Document]) -> List[ScientificArticle]:
        """
        Traite une liste de documents.
        
        Args:
            documents: Liste de documents LangChain
            
        Returns:
            Liste d'objets ScientificArticle
        """
        results = []
        failed_documents = []
        
        for i, document in enumerate(documents):
            try:
                result = self.process_document(document)
                results.append(result)
                print(f"Document {i+1}/{len(documents)} trait√© avec succ√®s")
            except Exception as e:
                failed_documents.append((i, str(e)))
                print(f"Erreur lors du traitement du document {i+1}: {e}")
        
        if failed_documents:
            print(f"\nAttention: {len(failed_documents)} documents ont √©chou√©:")
            for idx, error in failed_documents:
                print(f"  - Document {idx+1}: {error}")
        
        print(f"\nTraitement termin√©: {len(results)}/{len(documents)} documents trait√©s avec succ√®s")
        return results
    
    def batch_process_with_retry(self, documents: List[Document], 
                                max_retries: Optional[int] = None) -> List[ScientificArticle]:
        """
        Traite les documents par batch avec m√©canisme de retry.
        
        Args:
            documents: Liste de documents √† traiter
            max_retries: Nombre maximum de tentatives par document (utilise config si None)
            
        Returns:
            Liste d'objets ScientificArticle
        """
        max_retries = max_retries or config.MAX_RETRIES
        results = []
        
        for i, document in enumerate(documents):
            success = False
            last_error = None
            
            for attempt in range(max_retries):
                try:
                    result = self.process_document(document)
                    results.append(result)
                    success = True
                    print(f"Document {i+1}/{len(documents)} trait√© avec succ√®s (tentative {attempt+1})")
                    break
                except Exception as e:
                    last_error = e
                    if attempt < max_retries - 1:
                        print(f"√âchec tentative {attempt+1} pour document {i+1}, retry...")
                    
            if not success:
                print(f"Document {i+1} √©chou√© apr√®s {max_retries} tentatives: {last_error}")
        
        return results
    
    def update_prompt_template(self, new_template: str) -> None:
        """
        Met √† jour le template de prompt.
        
        Args:
            new_template: Nouveau template de prompt
        """
        self.prompt_template = PromptTemplate(
            template=new_template,
            input_variables=["text"],
            partial_variables={
                "format_instructions": self.output_parser.get_format_instructions()
            }
        )
        
        # Recr√©er la cha√Æne
        self.chain = self.prompt_template | self.llm | self.output_parser
    
    def get_processing_stats(self, articles: List[ScientificArticle]) -> Dict[str, Any]:
        """
        G√©n√®re des statistiques sur les articles trait√©s.
        
        Args:
            articles: Liste d'articles trait√©s
            
        Returns:
            Dictionnaire avec les statistiques
        """
        if not articles:
            return {"total_articles": 0}
        
        # Calculs des statistiques
        total_articles = len(articles)
        articles_with_authors = len([a for a in articles if a.authors])
        articles_with_doi = len([a for a in articles if a.doi])
        articles_with_date = len([a for a in articles if a.publication_date])
        
        confidence_scores = [a.confidence_score for a in articles if a.confidence_score is not None]
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
        
        research_fields = [a.research_field for a in articles if a.research_field]
        unique_fields = len(set(research_fields))
        
        stats = {
            "total_articles": total_articles,
            "articles_with_authors": articles_with_authors,
            "articles_with_doi": articles_with_doi,
            "articles_with_publication_date": articles_with_date,
            "average_confidence_score": round(avg_confidence, 3),
            "unique_research_fields": unique_fields,
            "completion_rates": {
                "authors": round(articles_with_authors / total_articles * 100, 1),
                "doi": round(articles_with_doi / total_articles * 100, 1),
                "publication_date": round(articles_with_date / total_articles * 100, 1)
            }
        }
        
        return stats